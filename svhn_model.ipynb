{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model for SVHN Dataset 32 x 32 Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying SVHN dataset by using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from datetime import timedelta\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16.0, 4.0) # Set default figure size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TENSORBOARD_SUMMARIES_DIR = '/tmp/svhn_classifier_logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data ....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file as readonly\n",
    "h5f = h5py.File('SVHN_grey.h5', 'r')\n",
    "\n",
    "# Load the training, test and validation set\n",
    "X_train = h5f['X_train'][:]\n",
    "y_train = h5f['y_train'][:]\n",
    "X_test = h5f['X_test'][:]\n",
    "y_test = h5f['y_test'][:]\n",
    "X_val = h5f['X_val'][:]\n",
    "y_val = h5f['y_val'][:]\n",
    "\n",
    "# Close this file\n",
    "h5f.close()\n",
    "\n",
    "print('Training set', X_train.shape, y_train.shape)\n",
    "print('Validation set', X_val.shape, y_val.shape)\n",
    "print('Test set', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting n rows x m col\n",
    "` Plot random images from passed data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(images, nrows, ncols, cls_true, cls_pred=None):\n",
    "    \"\"\" Plot nrows * ncols images from images and annotate the images\n",
    "    \"\"\"\n",
    "    # Initialize the subplotgrid\n",
    "    fig, axes = plt.subplots(nrows, ncols)\n",
    "    \n",
    "    # Randomly select nrows * ncols images\n",
    "    rs = np.random.choice(images.shape[0], nrows*ncols)\n",
    "    \n",
    "    # For every axes object in the grid\n",
    "    for i, ax in zip(rs, axes.flat): \n",
    "        \n",
    "        # Predictions are not passed\n",
    "        if cls_pred is None:\n",
    "            title = \"True: {0}\".format(np.argmax(cls_true[i]))\n",
    "        \n",
    "        # When predictions are passed, display labels + predictions\n",
    "        else:\n",
    "            title = \"True: {0}, Pred: {1}\".format(np.argmax(cls_true[i]), cls_pred[i])  \n",
    "            \n",
    "        # Display the image\n",
    "        ax.imshow(images[i,:,:,0], cmap='binary')\n",
    "        \n",
    "        # Annotate the image\n",
    "        ax.set_title(title)\n",
    "        \n",
    "        # Do not overlay a grid\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_log_dir():\n",
    "    '''Clears the log files then creates new directories to place\n",
    "        the tensorbard log file.''' \n",
    "    if tf.gfile.Exists(TENSORBOARD_SUMMARIES_DIR):\n",
    "        tf.gfile.DeleteRecursively(TENSORBOARD_SUMMARIES_DIR)\n",
    "    tf.gfile.MakeDirs(TENSORBOARD_SUMMARIES_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Batch Dynamically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(X, y, batch_size=512):\n",
    "    for i in np.arange(0, y.shape[0], batch_size):\n",
    "        end = min(X.shape[0], i + batch_size)\n",
    "        yield(X[i:end],y[i:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Using placeholder var whose value can directly\n",
    "feed to the graph`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = 32*32\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Our application logic will be added here\n",
    "x = tf.placeholder(tf.float32, shape = [None, 32, 32, 1], name='Input_Data')\n",
    "y = tf.placeholder(tf.float32, shape = [None, 10], name='Input_Labels')\n",
    "y_cls = tf.argmax(y, 1)\n",
    "\n",
    "discard_rate = tf.placeholder(tf.float32, name='Discard_rate')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Architecture function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Basic ConvNet Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL] -> DROPOUT -> [FC -> RELU] -> FC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    \n",
    "      # Input Layer\n",
    "    input_layer = tf.reshape(features, [-1, 32, 32, 1], name='Reshaped_Input')\n",
    "\n",
    "      # Convolutional Layer #1\n",
    "    #with tf.name_scope('Conv1 Layer + ReLU'):\n",
    "    \n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=32,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "      # Pooling Layer #1\n",
    "    #with tf.name_scope('Pool1 Layer'):\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "\n",
    "      # Convolutional Layer #2 and Pooling Layer #2\n",
    "    #with tf.name_scope('Conv2 Layer + ReLU'): \n",
    "    conv2 = tf.layers.conv2d(\n",
    "          inputs=pool1,\n",
    "          filters=64,\n",
    "          kernel_size=[5, 5],\n",
    "          padding=\"same\",\n",
    "          activation=tf.nn.relu)\n",
    "        \n",
    "    #with tf.name_scope('Pool2 Layer'):\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "      # Dense Layer\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 8 * 8 * 64])\n",
    "    dense = tf.layers.dense(inputs=pool2_flat, units=256, activation=tf.nn.relu)\n",
    "    dropout = tf.layers.dropout(\n",
    "         inputs=dense, rate=discard_rate)\n",
    "\n",
    "      # Logits Layer\n",
    "    #with tf.name_scope('Logits Layer'):\n",
    "    logits = tf.layers.dense(inputs=dropout, units=10)\n",
    "\n",
    "    return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 2\n",
    "num_examples = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_log_dir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction and Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Loss calculated for each iteration using prediction\n",
    "from \"cnn_model_fn()\" results`\n",
    "\n",
    "`Optimizer used is  Adam Optimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with tf.name_scope('Model Prediction'):\n",
    "prediction = cnn_model_fn(x)\n",
    "prediction_cls = tf.argmax(prediction, 1)\n",
    "#with tf.name_scope('loss'):\n",
    "loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(\n",
    "   onehot_labels=y, logits=prediction))\n",
    "    #tf.summary.scalar('loss', loss)\n",
    "    \n",
    "#with tf.name_scope('Adam Optimizer'):\n",
    "optimizer = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Calculate total prediction and also\n",
    "cal accuracy of passed data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted class equals the true class of each image?\n",
    "correct_prediction = tf.equal(prediction_cls, y_cls)\n",
    "\n",
    "# Cast predictions to float and calculate the mean\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged_summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saver Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Used for storing and reterving variable pesent \n",
    "in graph`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "save_dir = 'checkpnts/'\n",
    "\n",
    "# Create directory if it does not exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "save_path = os.path.join(save_dir, 'svhn_single_greyscale')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restoring the variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saver.restore(sess=session, save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialized Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## No of example in each batch for updating weights\n",
    "batch_size = 512\n",
    "\n",
    "#Discarding or fuse % of neurons in Train mode\n",
    "discard_per = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#with tf.Session() as sess:\n",
    " #   sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "## To calculate total time of training\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "start_time = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    print ('Training .........')\n",
    "    epoch_loss = 0\n",
    "    print ()\n",
    "    print ('Epoch ', epoch+1 , ': ........ \\n')\n",
    "    step = 0   \n",
    "    \n",
    "    ## Training epochs ....\n",
    "    for (epoch_x , epoch_y) in get_batch(X_train, y_train, batch_size):\n",
    "        _, train_accu, c = sess.run([optimizer, accuracy, loss], feed_dict={x: epoch_x, y: epoch_y, discard_rate: discard_per})\n",
    "        train_loss.append(c)\n",
    "    \n",
    "        if(step%40 == 0):\n",
    "            print (\"Step:\", step, \".....\", \"\\nMini-Batch Loss   : \", c)\n",
    "            print('Mini-Batch Accuracy :' , train_accu*100.0, '%')\n",
    "\n",
    "            ## Validating prediction and summaries\n",
    "            accu = 0.0\n",
    "            for (epoch_x , epoch_y) in get_batch(X_val, y_val, 512):                            \n",
    "                correct, _c = sess.run([correct_prediction, loss], feed_dict={x: epoch_x, y: epoch_y, discard_rate: 0.0})\n",
    "                valid_loss.append(_c)\n",
    "                accu+= np.sum(correct[correct == True])\n",
    "            print('Validation Accuracy :' , accu*100.0/y_val.shape[0], '%')\n",
    "            print ()\n",
    "        step = step + 1\n",
    "\n",
    "\n",
    "    print ('Epoch', epoch+1, 'completed out of ', max_epochs)\n",
    "\n",
    "    \n",
    "## Calculate net time\n",
    "time_diff = time.time() - start_time\n",
    "\n",
    "## Testing prediction and summaries\n",
    "accu = 0.0\n",
    "for (epoch_x , epoch_y) in get_batch(X_test, y_test, 512):\n",
    "    correct = sess.run([correct_prediction], feed_dict={x: epoch_x, y: epoch_y, discard_rate: 0.0})\n",
    "    accu+= np.sum(correct[correct == True])\n",
    "print('Test Accuracy :' , accu*100.0/y_test.shape[0], '%')\n",
    "print(\"Time usage: \" + str(timedelta(seconds=int(round(time_diff)))))\n",
    "print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"accu = 0.0\n",
    "for (epoch_x , epoch_y) in get_batch(X_test, y_test, 512):\n",
    "    correct = sess.run([correct_prediction], feed_dict={x: epoch_x, y: epoch_y, discard_rate: 0.0})\n",
    "    predcls, classy = sess.run([prediction_cls, y_cls], feed_dict={x: epoch_x, y: epoch_y, discard_rate: 0.0})\n",
    "    accumulate = np.sum((predcls == classy)*1)\n",
    "    crct = np.sum(correct[correct == True])\n",
    "    deaccumulate = np.sum((predcls != classy)*1)\n",
    "    accu+= accumulate\n",
    "    print('Test Accuracy :' , accumulate, crct, deaccumulate, accumulate+ deaccumulate, y_test.shape[0], '%')\n",
    "print ()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** To save the model Checkpoints **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.save(sess=sess, save_path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Plot different confusion map and images **\n",
    "\n",
    "** to get insights of how our model is performing **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Plotting Train Actual Images\n",
    "` Plot 3 rows x 6 col images randomly\n",
    "with their true labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(X_train, 3, 6, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Evaluate predicted class from test data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_pred = []\n",
    "for (epoch_x , epoch_y) in get_batch(X_test, y_test, 512):\n",
    "    correct = sess.run([prediction_cls], feed_dict={x: epoch_x, y: epoch_y, discard_rate: 0.0})\n",
    "    test_pred.append((np.asarray(correct, dtype=int)).T)\n",
    "\n",
    "print ('Completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Lists of List to numpy array\n",
    "` compatiable with y_label`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(lists):\n",
    "    results = []\n",
    "    for numbers in lists:\n",
    "        for x in numbers:\n",
    "            results.append(x)\n",
    "    return np.asarray(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_array = flatten(test_pred)\n",
    "flat_array = (flat_array.T)\n",
    "flat_array = flat_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**I got that concept wrong in one interview, so kindly search about it; \n",
    ">**\n",
    "\n",
    "`Plot and see the misclassfied results of model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_true=np.argmax(y_test, axis=1), y_pred=flat_array)\n",
    "\n",
    "# Normalize the confusion matrix\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100.0\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "sns.heatmap(cm, annot=True, cmap='Reds', fmt='.1f', square=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Misclassified results\n",
    "\n",
    "`Plot the misclassfied images of randomly sampled\n",
    "from test set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the incorrectly classified examples\n",
    "incorrect = flat_array != np.argmax(y_test, axis=1)\n",
    "\n",
    "# Select the incorrectly classified examples\n",
    "images = X_test[incorrect]\n",
    "cls_true = y_test[incorrect]\n",
    "cls_pred = flat_array[incorrect]\n",
    "\n",
    "# Plot the mis-classified examples\n",
    "plot_images(images, 3, 6, cls_true, cls_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Correctly classified results\n",
    "\n",
    "`Plot the correctly classfied images of randomly sampled\n",
    "from test set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the incorrectly classified examples\n",
    "correct = np.invert(incorrect)\n",
    "\n",
    "# Select the correctly classified examples\n",
    "images = X_test[correct]\n",
    "cls_true = y_test[correct]\n",
    "cls_pred = flat_array[correct]\n",
    "\n",
    "# Plot the mis-classified examples\n",
    "plot_images(images, 3, 6, cls_true, cls_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Validation and Training Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Plotting the losses to gurantee that model\n",
    "is learning good or not`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss ,'r')\n",
    "plt.plot(valid_loss, 'g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
